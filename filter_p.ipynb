{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUXKh8UVGv1I",
        "outputId": "cfd79363-f0cd-4204-a9d5-3219fa423fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import pdb\n",
        "from os import getcwd\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "4GUm2SM9G37r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading CIFAR10 train and test dataset\n",
        "def prep_dataloaders():\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True,\n",
        "                                            transform=train_transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                              shuffle=True)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True,\n",
        "                                           transform=test_transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                             shuffle=False)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "               'ship', 'truck')\n",
        "\n",
        "    return trainloader, testloader, classes\n",
        "trainloader, testloader, classes = prep_dataloaders()\n",
        "\n",
        "# Print the results\n",
        "# print(\"Trainloader:\", trainloader)\n",
        "# print(\"Testloader:\", testloader)\n",
        "# print(\"Classes:\", classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp3HNl70G5cm",
        "outputId": "367cf47e-7cf6-43c4-cf8e-35b4f1805370"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 41627500.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model to be used for training\n",
        "# MaskedConv2d is a subclass of nn.Conv2d for a masking mechanism\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=False):\n",
        "        super(MaskedConv2d, self).__init__(in_channels, out_channels,\n",
        "                                           kernel_size, stride, padding,\n",
        "                                           dilation, groups, bias)\n",
        "        self.masked_channels = []\n",
        "        self.mask_flag = False\n",
        "        self.masks = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.mask_flag:\n",
        "            self._expand_masks(x.size())\n",
        "            weight = self.weight * self.masks\n",
        "            return F.conv2d(x, weight, self.bias, self.stride, self.padding,\n",
        "                            self.dilation, self.groups)\n",
        "        else:\n",
        "            return F.conv2d(x, self.weight, self.bias, self.stride,\n",
        "                            self.padding, self.dilation, self.groups)\n",
        "\n",
        "    def set_masked_channels(self, masked_channels):\n",
        "        self.masked_channels = masked_channels\n",
        "        self.mask_flag = len(masked_channels) > 0\n",
        "\n",
        "    def get_masked_channels(self):\n",
        "        return self.masked_channels\n",
        "\n",
        "    def _expand_masks(self, input_size):\n",
        "        if not self.masked_channels:\n",
        "            self.masks = None\n",
        "        else:\n",
        "            batch_size, _, height, width = [int(input_size[i].item()) for i in range(4)]\n",
        "            masks = torch.ones((len(self.masked_channels), batch_size, height, width), device=self.weight.device)\n",
        "            self.masks = Variable(masks, requires_grad=False)"
      ],
      "metadata": {
        "id": "3YKkqvRnG6Ro"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Neural network architecture\n",
        "class CustomNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomNet, self).__init__()\n",
        "        self.conv1_1 = MaskedConv2d(3, 64, 3, padding=1)\n",
        "        self.conv2_1 = MaskedConv2d(64, 128, 3, padding=1)\n",
        "        self.conv3_1 = MaskedConv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(4096, 4096)\n",
        "        self.fc2 = nn.Linear(4096, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1_1(x))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "\n",
        "        out = F.relu(self.conv2_1(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "\n",
        "        out = F.relu(self.conv3_1(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        return self.softmax(out)"
      ],
      "metadata": {
        "id": "LfEh1RQBG6XX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, trainloader):\n",
        "    \"\"\"A single training iteration\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Batch loop\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # A single optimization step\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update loss and accuracy info\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(\"Train: Loss: %.3f, Acc: %.3f (%d/%d)\" % (train_loss / (batch_idx + 1), correct / total * 100., correct, total))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def test(model, optimizer, criterion, testloader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Batch loop\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # A single test iteration\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Update loss and accuracy info\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(\"Test: Loss: %.3f, Acc: %.3f (%d/%d)\" % (test_loss / (batch_idx + 1), correct / total * 100., correct, total))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "E5Ngnrh5KUXE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set up model, optimizer and loss\n",
        "model = CustomNet(len(classes))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# set learning rate = 0.001\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# print(criterion)\n",
        "\n",
        "# scale (multiply) the learning rate of an optimizer for adjusting the learning rate during training\n",
        "def scale_lr(optimizer, scale):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] *= scale\n"
      ],
      "metadata": {
        "id": "RHoA78d2MYi6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_train_acc = 0\n",
        "best_test_acc = 0\n",
        "# Number of training epochs for pretraining\n",
        "num_pretrain_epochs = 10\n",
        "\n",
        "\n",
        "# Loop over training epochs\n",
        "for epoch in range(num_pretrain_epochs):\n",
        "\n",
        "    print(\"========== epoch %d\" % (epoch))\n",
        "\n",
        "    # Update learning rate of optimizers regularly\n",
        "    if epoch % 50 == 0:\n",
        "          scale_lr(optimizer, 0.1)\n",
        "\n",
        "    # Train\n",
        "    tic = time.time()\n",
        "    train_acc = train(model, optimizer, criterion, trainloader)\n",
        "    print(\"Train Time: %.3f\" % (time.time() - tic))\n",
        "    if train_acc > best_train_acc:\n",
        "          best_train_acc = train_acc\n",
        "\n",
        "    # Evaluate\n",
        "    tic = time.time()\n",
        "    test_acc = test(model, optimizer, criterion, trainloader)\n",
        "    print(\"Test Time: %.3f\" % (time.time() - tic))\n",
        "    if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "print(\"Best Training Accuracy: %.3f%%\" % (best_train_acc * 100.))\n",
        "print(\"Best Test Accuracy: %.3f%%\" % (best_test_acc * 100.))\n",
        "\n",
        "# Save model and optimizer as checkpoint\n",
        "save_dir = getcwd() + \"/saved_model.pth\"\n",
        "save_data = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    }\n",
        "torch.save(save_data, save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfBO69dHNvuS",
        "outputId": "c2b5e35f-d97a-4f21-f657-d316b6df2466"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== epoch 0\n",
            "Train: Loss: 1.857, Acc: 61.460 (30730/50000)\n",
            "Train Time: 21.929\n",
            "Test: Loss: 1.853, Acc: 61.762 (30881/50000)\n",
            "Test Time: 16.482\n",
            "========== epoch 1\n",
            "Train: Loss: 1.851, Acc: 61.894 (30947/50000)\n",
            "Train Time: 21.761\n",
            "Test: Loss: 1.851, Acc: 61.958 (30979/50000)\n",
            "Test Time: 16.474\n",
            "========== epoch 2\n",
            "Train: Loss: 1.851, Acc: 61.988 (30994/50000)\n",
            "Train Time: 21.605\n",
            "Test: Loss: 1.849, Acc: 62.308 (31154/50000)\n",
            "Test Time: 16.359\n",
            "========== epoch 3\n",
            "Train: Loss: 1.850, Acc: 62.150 (31075/50000)\n",
            "Train Time: 21.632\n",
            "Test: Loss: 1.847, Acc: 62.402 (31201/50000)\n",
            "Test Time: 16.540\n",
            "========== epoch 4\n",
            "Train: Loss: 1.848, Acc: 62.454 (31227/50000)\n",
            "Train Time: 21.570\n",
            "Test: Loss: 1.846, Acc: 62.542 (31271/50000)\n",
            "Test Time: 16.123\n",
            "========== epoch 5\n",
            "Train: Loss: 1.846, Acc: 62.552 (31276/50000)\n",
            "Train Time: 21.432\n",
            "Test: Loss: 1.845, Acc: 62.682 (31341/50000)\n",
            "Test Time: 16.877\n",
            "========== epoch 6\n",
            "Train: Loss: 1.845, Acc: 62.810 (31405/50000)\n",
            "Train Time: 21.653\n",
            "Test: Loss: 1.848, Acc: 62.366 (31183/50000)\n",
            "Test Time: 16.322\n",
            "========== epoch 7\n",
            "Train: Loss: 1.846, Acc: 62.588 (31294/50000)\n",
            "Train Time: 21.636\n",
            "Test: Loss: 1.843, Acc: 63.048 (31524/50000)\n",
            "Test Time: 17.388\n",
            "========== epoch 8\n",
            "Train: Loss: 1.843, Acc: 62.896 (31448/50000)\n",
            "Train Time: 21.551\n",
            "Test: Loss: 1.843, Acc: 62.874 (31437/50000)\n",
            "Test Time: 16.062\n",
            "========== epoch 9\n",
            "Train: Loss: 1.843, Acc: 63.072 (31536/50000)\n",
            "Train Time: 21.443\n",
            "Test: Loss: 1.840, Acc: 63.374 (31687/50000)\n",
            "Test Time: 16.215\n",
            "Best Training Accuracy: 63.072%\n",
            "Best Test Accuracy: 63.374%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Params():\n",
        "    def __init__(self):\n",
        "\n",
        "        # Number of training epochs for sparse learning\n",
        "        self.num_sparse_train_epochs = 10\n",
        "\n",
        "        # Learning rate for optimizer\n",
        "        self.learning_rate = 0.001\n",
        "\n",
        "        # Hyperparameters for structured sparsity learning\n",
        "        self.ssl_hyperparams = {\n",
        "            \"wgt_decay\": 5e-4,\n",
        "            \"lambda_n\": 5e-2,\n",
        "            \"lambda_c\": 5e-2,\n",
        "            \"lambda_s\": 5e-2,\n",
        "        }\n",
        "\n",
        "        # Threshold below which a weight value should be counted as too low\n",
        "        self.threshold = 1e-5\n"
      ],
      "metadata": {
        "id": "l9wNhU87X37L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S2LVEdCAoMtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SSL can be applied on the pretrained model\n",
        "\n",
        "def group_lasso(param_group):\n",
        "    return torch.sum(param_group ** 2)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(outputs, targets):\n",
        "    \"\"\"Cross-entropy loss\"\"\"\n",
        "    ce_loss_func = nn.CrossEntropyLoss()\n",
        "    return ce_loss_func(outputs, targets)\n",
        "\n",
        "\n",
        "def filter_and_channel_wise_ssl_loss(model, outputs, targets, params):\n",
        "    \"\"\"\n",
        "    Penalizing unimportant filters and channels\n",
        "\n",
        "    Params:\n",
        "        - pretrained model\n",
        "        - output tensor generated by the forward step of model\n",
        "        - target output tensor corresponding to the input (for CE loss)\n",
        "        - hyperparameters which contain entry for weight decay, lambda_n\n",
        "        (for filter-wise group LASSO), and lambda_c (for channel-wise group LASSO)\n",
        "\n",
        "    Return:\n",
        "        - Loss of \"Learning Structured Sparsity in Deep Neural Networks\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute cross-entropy loss\n",
        "    ce_loss = cross_entropy_loss(outputs, targets)\n",
        "\n",
        "    # Loss accumulators\n",
        "    wgt_l2_norm = torch.Tensor([0.])\n",
        "    filter_wise_loss = torch.Tensor([0.])\n",
        "    channel_wise_loss = torch.Tensor([0.])\n",
        "\n",
        "# Check for GPU availability and move tensors accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        wgt_l2_norm = wgt_l2_norm.cuda()\n",
        "        filter_wise_loss = filter_wise_loss.cuda()\n",
        "        channel_wise_loss = channel_wise_loss.cuda()\n",
        "\n",
        "    # Coefficient hyperparams\n",
        "    hyperparams = params.ssl_hyperparams\n",
        "    wgt_decay = hyperparams[\"wgt_decay\"]\n",
        "    lambda_n = hyperparams[\"lambda_n\"]\n",
        "    lambda_c = hyperparams[\"lambda_c\"]\n",
        "\n",
        "    # Iterate over every layer\n",
        "    params = list(model.parameters())\n",
        "    for param in params:\n",
        "        # L2 norm over entire parameters\n",
        "        wgt_l2_norm += torch.norm(param)\n",
        "\n",
        "        # Ignore linear or bias parameters\n",
        "        if len(param.size()) != 4:\n",
        "            continue\n",
        "\n",
        "        num_filters, num_channels = param.size()[0], param.size()[1]\n",
        "\n",
        "        # Group LASSO over filters of current layer\n",
        "        for filter_idx in range(num_filters):\n",
        "            filter_wise_loss += group_lasso(param[filter_idx, :, :, :])\n",
        "\n",
        "        # Group LASSO over channels of current layer\n",
        "        for channel_idx in range(num_channels):\n",
        "            channel_wise_loss += group_lasso(param[:, channel_idx, :, :])\n",
        "\n",
        "    return ce_loss + (wgt_decay * wgt_l2_norm) + (lambda_n * \\\n",
        "           filter_wise_loss)+ (lambda_c * channel_wise_loss)\n",
        "\n",
        "\n",
        "def prep_dataloaders():\n",
        "    \"\"\"Loads CIFAR10 train and test dataset\"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                            download=True,\n",
        "                                            transform=train_transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                              shuffle=True)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True,\n",
        "                                           transform=test_transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                             shuffle=False)\n",
        "\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "               'ship', 'truck')\n",
        "\n",
        "    return trainloader, testloader, classes\n",
        "\n",
        "\n",
        "\n",
        "def train_ssl(model, optimizer, ssl_loss_func, trainloader, params):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "    # Check for GPU availability and move tensors accordingly\n",
        "     if torch.cuda.is_available():\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = ssl_loss_func(model, outputs, targets, params)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "\n",
        "    print(\"Train: Loss: %.3f, Acc: %.3f (%d/%d)\" % (train_loss / \\\n",
        "          (batch_idx + 1), correct / total * 100., correct, total))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def test(model, optimizer, criterion, testloader):\n",
        "   model.eval()\n",
        "   test_loss = 0\n",
        "   correct = 0\n",
        "   total = 0\n",
        "\n",
        "   for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "    # Check for GPU availability and move tensors accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "\n",
        "    print(\"Test: Loss: %.3f, Acc: %.3f (%d/%d)\" % (test_loss / \\\n",
        "          (batch_idx + 1), correct / total * 100., correct, total))\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def count_sparse_wgt(model, threshold):\n",
        "    weight_cnt = 0\n",
        "    sparse_weight_cnt = 0\n",
        "    with torch.no_grad():\n",
        "        for param_key in model.state_dict():\n",
        "            param_tensor = model.state_dict()[param_key]\n",
        "            dims = 1\n",
        "            for dim in list(param_tensor.size()):\n",
        "                dims *= dim\n",
        "            weight_cnt += dims\n",
        "            sparse_weight_cnt += torch.sum(param_tensor < threshold).item()\n",
        "    return weight_cnt, sparse_weight_cnt\n",
        "\n",
        "\n",
        "def count_sparse_wgt_by_layer(model, threshold):\n",
        "    wgt_cnts = []\n",
        "    sparse_wgt_cnts = []\n",
        "    with torch.no_grad():\n",
        "        for param_key in model.state_dict():\n",
        "            param_tensor = model.state_dict()[param_key]\n",
        "            dims = 1\n",
        "            for dim in list(param_tensor.size()):\n",
        "                dims *= dim\n",
        "            wgt_cnts.append((param_key, dims))\n",
        "            sparse_wgt_cnt_layer = torch.sum(param_tensor < threshold).item()\n",
        "            sparse_wgt_cnts.append((param_key, sparse_wgt_cnt_layer))\n",
        "    return wgt_cnts, sparse_wgt_cnts\n",
        "\n",
        "\n",
        "def count_sparse_wgt_by_filter(model, threshold):\n",
        "    sparse_wgt_cnts = []\n",
        "    with torch.no_grad():\n",
        "        for param_key in model.state_dict():\n",
        "            param_tensor = model.state_dict()[param_key]\n",
        "            if len(param_tensor.size()) != 4:\n",
        "                sparse_wgt_cnts.append((param_key, None))\n",
        "                continue\n",
        "            num_filters = param_tensor.size()[0]\n",
        "            sparse_wgt_cnts_by_filter = []\n",
        "            for filter_idx in range(num_filters):\n",
        "                cnt = torch.sum(param_tensor[filter_idx, :, :, :] < \\\n",
        "                                threshold).item()\n",
        "                sparse_wgt_cnts_by_filter.append(cnt)\n",
        "            sparse_wgt_cnts.append((param_key, sparse_wgt_cnts_by_filter))\n",
        "    return sparse_wgt_cnts\n",
        "\n",
        "\n",
        "def count_sparse_wgt_by_channel(model, threshold):\n",
        "    sparse_wgt_cnts = []\n",
        "    with torch.no_grad():\n",
        "        for param_key in model.state_dict():\n",
        "            param_tensor = model.state_dict()[param_key]\n",
        "            if len(param_tensor.size()) != 4:\n",
        "                sparse_wgt_cnts.append((param_key, None))\n",
        "                continue\n",
        "            num_channels = param_tensor.size()[1]\n",
        "            sparse_wgt_cnts_by_channel = []\n",
        "            for channel_idx in range(num_channels):\n",
        "                cnt = torch.sum(param_tensor[:, channel_idx, :, :] < \\\n",
        "                                threshold).item()\n",
        "                sparse_wgt_cnts_by_channel.append(cnt)\n",
        "            sparse_wgt_cnts.append((param_key, sparse_wgt_cnts_by_channel))\n",
        "    return sparse_wgt_cnts\n",
        "\n",
        "\n",
        "def print_sparse_weights(model, threshold):\n",
        "    wgt_cnt, sparse_wgt_cnt = count_sparse_wgt(model, threshold)\n",
        "    print(\"\\nTotal sparse weights: %.3f (%d/%d)\" % (100. * sparse_wgt_cnt / \\\n",
        "          wgt_cnt, sparse_wgt_cnt, wgt_cnt))\n",
        "\n",
        "    wgt_cnts, sparse_wgt_cnts = count_sparse_wgt_by_layer(model, threshold)\n",
        "    print(\"\\nSparse weight by layer\")\n",
        "    for idx in range(len(wgt_cnts)):\n",
        "        layer_name = wgt_cnts[idx][0]\n",
        "        wgt_cnt = wgt_cnts[idx][1]\n",
        "        sparse_wgt_cnt = sparse_wgt_cnts[idx][1]\n",
        "        print(\"Layer: {}, {} ({}/{})\".format(layer_name, sparse_wgt_cnt / \\\n",
        "              wgt_cnt, sparse_wgt_cnt, wgt_cnt))\n",
        "\n",
        "    sparse_wgt_cnts = count_sparse_wgt_by_filter(model, threshold)\n",
        "    print(\"\\nSparse weight by filter\")\n",
        "    for idx in range(len(sparse_wgt_cnts)):\n",
        "        layer_name = sparse_wgt_cnts[idx][0]\n",
        "        wgts_filters = sparse_wgt_cnts[idx][1]\n",
        "        print(\"Layer: {}, {}\".format(layer_name, wgts_filters))\n",
        "\n",
        "\n",
        "\n",
        "params = Params()\n",
        "\n",
        "trainloader, testloader, classes = prep_dataloaders()\n",
        "model = CustomNet(len(classes))\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=params.learning_rate, momentum=0.9,\n",
        "#                 weight_decay=5e-4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params.learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "checkpoint = torch.load(getcwd() + \"/saved_model.pth\")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "print_sparse_weights(model, params.threshold)\n",
        "\n",
        "best_train_acc = 0\n",
        "best_test_acc = 0\n",
        "\n",
        "for epoch in range(params.num_sparse_train_epochs):\n",
        "        print(\"========== epoch %d\" % (epoch))\n",
        "        # if epoch % 50 == 0:\n",
        "        #     scale_lr(optimizer, 0.1)\n",
        "\n",
        "\n",
        "        ssl_loss_func = filter_and_channel_wise_ssl_loss\n",
        "\n",
        "\n",
        "        tic = time.time()\n",
        "        train_acc = train_ssl(model, optimizer, ssl_loss_func, trainloader,\n",
        "                              params)\n",
        "        print(\"Train Time: %.3f\" % (time.time() - tic))\n",
        "        if train_acc > best_train_acc:\n",
        "            best_train_acc = train_acc\n",
        "\n",
        "        tic = time.time()\n",
        "        test_acc = test(model, optimizer, criterion, trainloader)\n",
        "        print(\"Test Time: %.3f\" % (time.time() - tic))\n",
        "        if test_acc > best_test_acc:\n",
        "            best_test_acc = test_acc\n",
        "\n",
        "print(\"Best Training Accuracy: %.3f%%\" % (best_train_acc * 100.))\n",
        "print(\"Best Test Accuracy: %.3f%%\" % (best_test_acc * 100.))\n",
        "\n",
        "print_sparse_weights(model, params.threshold)\n",
        "test_acc = test(model, optimizer, criterion, trainloader)\n",
        "print(\"Final test accuracy: {}\".format(test_acc * 100.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkQ2Sa3OUW0G",
        "outputId": "9410bfff-36d2-467f-d74c-073c87ee0418"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Total sparse weights: 74.103 (12740248/17192650)\n",
            "\n",
            "Sparse weight by layer\n",
            "Layer: conv1_1.weight, 0.47858796296296297 (827/1728)\n",
            "Layer: conv2_1.weight, 0.5255940755208334 (38751/73728)\n",
            "Layer: conv3_1.weight, 0.5858629014756944 (172778/294912)\n",
            "Layer: fc1.weight, 0.7453654408454895 (12505157/16777216)\n",
            "Layer: fc1.bias, 0.685302734375 (2807/4096)\n",
            "Layer: fc2.weight, 0.486376953125 (19922/40960)\n",
            "Layer: fc2.bias, 0.6 (6/10)\n",
            "\n",
            "Sparse weight by filter\n",
            "Layer: conv1_1.weight, [12, 13, 14, 7, 12, 12, 15, 10, 14, 11, 13, 14, 14, 12, 13, 13, 14, 15, 12, 12, 14, 13, 13, 13, 14, 14, 15, 13, 14, 11, 12, 15, 12, 5, 13, 16, 12, 13, 13, 14, 14, 12, 10, 11, 27, 14, 14, 11, 12, 15, 13, 14, 7, 13, 11, 15, 12, 13, 14, 12, 10, 16, 13, 13]\n",
            "Layer: conv2_1.weight, [296, 576, 332, 379, 309, 319, 279, 214, 384, 345, 358, 309, 355, 277, 287, 274, 296, 282, 248, 316, 285, 297, 223, 297, 248, 326, 316, 377, 230, 264, 254, 290, 316, 309, 328, 273, 254, 243, 262, 255, 246, 347, 242, 272, 248, 326, 291, 325, 258, 281, 292, 313, 226, 229, 341, 309, 258, 380, 282, 237, 214, 275, 272, 328, 294, 576, 378, 316, 302, 354, 309, 258, 278, 249, 319, 349, 229, 308, 224, 347, 315, 282, 261, 261, 214, 271, 346, 576, 302, 576, 255, 298, 275, 242, 330, 330, 285, 272, 341, 369, 275, 359, 292, 250, 262, 351, 381, 255, 399, 302, 223, 245, 285, 306, 289, 255, 281, 242, 271, 305, 310, 320, 308, 358, 319, 337, 279, 327]\n",
            "Layer: conv3_1.weight, [661, 572, 592, 606, 634, 1152, 659, 609, 560, 596, 627, 597, 627, 674, 550, 1152, 578, 612, 1152, 1152, 1139, 570, 589, 580, 601, 617, 642, 586, 600, 573, 578, 566, 606, 685, 610, 681, 588, 593, 639, 577, 623, 565, 672, 607, 518, 1152, 625, 582, 591, 558, 703, 594, 653, 546, 584, 651, 664, 588, 636, 649, 657, 648, 583, 639, 663, 605, 639, 657, 582, 570, 626, 580, 625, 588, 607, 629, 578, 595, 577, 592, 642, 623, 643, 633, 676, 586, 638, 605, 661, 591, 568, 559, 602, 613, 1152, 621, 648, 607, 636, 1152, 601, 556, 578, 1144, 655, 576, 622, 622, 615, 614, 629, 635, 602, 668, 615, 1152, 568, 588, 569, 634, 597, 659, 576, 628, 684, 557, 627, 661, 573, 597, 648, 593, 698, 605, 618, 627, 607, 599, 583, 602, 636, 637, 645, 566, 633, 628, 568, 597, 611, 621, 568, 564, 1152, 1145, 570, 595, 1152, 610, 650, 1152, 646, 612, 588, 553, 677, 641, 637, 566, 599, 572, 625, 1152, 1152, 697, 707, 607, 642, 578, 1152, 609, 602, 598, 626, 606, 603, 599, 597, 556, 1152, 602, 672, 604, 569, 642, 671, 1152, 591, 681, 654, 594, 599, 657, 635, 602, 663, 581, 1152, 633, 1152, 636, 1152, 769, 626, 1152, 1152, 633, 591, 597, 639, 593, 649, 647, 1152, 590, 614, 581, 607, 634, 1152, 669, 622, 585, 625, 1152, 603, 667, 674, 585, 591, 586, 1152, 632, 587, 614, 606, 600, 614, 1152, 594, 604, 609, 634, 690, 581, 625, 581]\n",
            "Layer: fc1.weight, None\n",
            "Layer: fc1.bias, None\n",
            "Layer: fc2.weight, None\n",
            "Layer: fc2.bias, None\n",
            "========== epoch 0\n",
            "Train: Loss: 0.035, Acc: 61.250 (49/80)\n",
            "Train Time: 15.607\n",
            "Test: Loss: 1.832, Acc: 64.062 (82/128)\n",
            "Test Time: 0.042\n",
            "========== epoch 1\n",
            "Train: Loss: 0.035, Acc: 73.750 (59/80)\n",
            "Train Time: 13.637\n",
            "Test: Loss: 1.809, Acc: 66.406 (85/128)\n",
            "Test Time: 0.039\n",
            "========== epoch 2\n",
            "Train: Loss: 0.035, Acc: 62.500 (50/80)\n",
            "Train Time: 13.981\n",
            "Test: Loss: 1.913, Acc: 56.250 (72/128)\n",
            "Test Time: 0.037\n",
            "========== epoch 3\n",
            "Train: Loss: 0.035, Acc: 66.250 (53/80)\n",
            "Train Time: 15.487\n",
            "Test: Loss: 1.805, Acc: 66.406 (85/128)\n",
            "Test Time: 0.063\n",
            "========== epoch 4\n",
            "Train: Loss: 0.035, Acc: 60.000 (48/80)\n",
            "Train Time: 13.861\n",
            "Test: Loss: 1.880, Acc: 57.812 (74/128)\n",
            "Test Time: 0.041\n",
            "========== epoch 5\n",
            "Train: Loss: 0.035, Acc: 61.250 (49/80)\n",
            "Train Time: 14.335\n",
            "Test: Loss: 1.845, Acc: 61.719 (79/128)\n",
            "Test Time: 0.041\n",
            "========== epoch 6\n",
            "Train: Loss: 0.035, Acc: 70.000 (56/80)\n",
            "Train Time: 14.112\n",
            "Test: Loss: 1.739, Acc: 76.562 (98/128)\n",
            "Test Time: 0.038\n",
            "========== epoch 7\n",
            "Train: Loss: 0.035, Acc: 52.500 (42/80)\n",
            "Train Time: 15.911\n",
            "Test: Loss: 1.834, Acc: 63.281 (81/128)\n",
            "Test Time: 0.047\n",
            "========== epoch 8\n",
            "Train: Loss: 0.035, Acc: 66.250 (53/80)\n",
            "Train Time: 13.977\n",
            "Test: Loss: 1.825, Acc: 64.062 (82/128)\n",
            "Test Time: 0.038\n",
            "========== epoch 9\n",
            "Train: Loss: 0.035, Acc: 62.500 (50/80)\n",
            "Train Time: 13.935\n",
            "Test: Loss: 1.818, Acc: 66.406 (85/128)\n",
            "Test Time: 0.041\n",
            "Best Training Accuracy: 73.750%\n",
            "Best Test Accuracy: 76.562%\n",
            "\n",
            "Total sparse weights: 74.089 (12737855/17192650)\n",
            "\n",
            "Sparse weight by layer\n",
            "Layer: conv1_1.weight, 0.4791666666666667 (828/1728)\n",
            "Layer: conv2_1.weight, 0.5214029947916666 (38442/73728)\n",
            "Layer: conv3_1.weight, 0.5862291124131944 (172886/294912)\n",
            "Layer: fc1.weight, 0.7452343702316284 (12502958/16777216)\n",
            "Layer: fc1.bias, 0.68505859375 (2806/4096)\n",
            "Layer: fc2.weight, 0.4865478515625 (19929/40960)\n",
            "Layer: fc2.bias, 0.6 (6/10)\n",
            "\n",
            "Sparse weight by filter\n",
            "Layer: conv1_1.weight, [12, 13, 14, 7, 12, 12, 15, 10, 14, 11, 14, 14, 14, 12, 13, 13, 14, 15, 12, 12, 14, 13, 13, 13, 14, 14, 15, 13, 14, 11, 12, 15, 12, 5, 13, 16, 12, 13, 13, 14, 14, 12, 10, 11, 27, 14, 14, 11, 12, 15, 13, 14, 7, 13, 11, 15, 12, 13, 14, 12, 10, 16, 13, 13]\n",
            "Layer: conv2_1.weight, [296, 576, 331, 379, 307, 319, 279, 214, 384, 345, 358, 309, 355, 279, 286, 274, 296, 282, 248, 316, 285, 298, 223, 297, 247, 326, 316, 377, 229, 264, 254, 290, 315, 309, 328, 273, 254, 243, 253, 254, 246, 347, 242, 272, 248, 326, 291, 326, 258, 281, 292, 313, 226, 229, 341, 309, 259, 379, 281, 235, 214, 275, 272, 328, 294, 352, 378, 316, 302, 354, 309, 258, 278, 249, 319, 349, 229, 308, 224, 347, 317, 282, 261, 260, 214, 271, 346, 513, 302, 576, 255, 298, 275, 242, 330, 329, 285, 272, 341, 369, 276, 359, 292, 250, 262, 351, 381, 248, 399, 298, 223, 245, 285, 306, 290, 255, 281, 245, 271, 305, 310, 320, 308, 358, 319, 336, 280, 327]\n",
            "Layer: conv3_1.weight, [662, 572, 590, 608, 633, 1152, 662, 609, 577, 606, 625, 605, 627, 673, 561, 1152, 581, 608, 1127, 1152, 1084, 570, 588, 581, 606, 606, 638, 594, 603, 579, 594, 575, 606, 686, 609, 670, 591, 586, 647, 585, 622, 565, 673, 609, 526, 1152, 627, 592, 586, 566, 711, 595, 650, 552, 591, 645, 664, 594, 636, 648, 655, 648, 584, 638, 672, 596, 639, 656, 582, 587, 634, 582, 625, 596, 598, 643, 571, 586, 585, 586, 638, 621, 636, 643, 664, 579, 635, 617, 667, 595, 569, 560, 603, 613, 1152, 621, 648, 623, 625, 1152, 613, 555, 580, 1019, 644, 577, 611, 629, 613, 623, 630, 651, 608, 667, 615, 1152, 568, 591, 572, 634, 610, 659, 582, 620, 692, 562, 621, 658, 568, 591, 657, 590, 696, 606, 618, 626, 616, 597, 591, 595, 633, 633, 643, 559, 636, 632, 575, 592, 610, 613, 576, 562, 1152, 1104, 570, 603, 1152, 610, 661, 1152, 648, 622, 585, 559, 677, 642, 628, 566, 611, 580, 622, 1152, 1152, 704, 707, 598, 634, 584, 1152, 601, 614, 600, 634, 606, 608, 599, 601, 567, 1135, 600, 672, 608, 572, 641, 678, 1152, 599, 692, 659, 601, 608, 653, 620, 616, 671, 585, 1152, 645, 1152, 645, 1152, 770, 624, 1152, 1152, 639, 593, 612, 648, 595, 652, 662, 1152, 595, 619, 581, 608, 645, 1152, 687, 611, 587, 629, 1152, 603, 671, 683, 577, 609, 593, 1071, 626, 585, 621, 617, 600, 614, 1152, 596, 602, 618, 637, 669, 581, 618, 582]\n",
            "Layer: fc1.weight, None\n",
            "Layer: fc1.bias, None\n",
            "Layer: fc2.weight, None\n",
            "Layer: fc2.bias, None\n",
            "Test: Loss: 1.857, Acc: 64.062 (82/128)\n",
            "Final test accuracy: 64.0625\n"
          ]
        }
      ]
    }
  ]
}