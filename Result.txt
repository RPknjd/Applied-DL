Result of weight pruning:
Size of the unpruned model before compression: 4.11 Mb
Size of the unpruned model after compression: 3.79 Mb
Size of the pruned model before compression: 16.28 Mb
Size of the pruned model after compression: 3.27 Mb

Unpruned Model:
Test loss: 0.9289465546607971
Test accuracy: 0.6873999834060669

Pruned Model:
Test loss: 0.9116255044937134
Test accuracy: 0.7014999985694885
313/313 [==============================] - 1s 2ms/step
313/313 [==============================] - 1s 2ms/step




Result of training model without applying Filter pruning:
Test: Loss: 1.840, Acc: 63.374 (31687/50000)
Test Time: 16.215
Best Training Accuracy: 63.072%
Best Test Accuracy: 63.374%



Result of training model with Filter pruning and group lasso regularization:
Total sparse weights: 74.103 (12740248/17192650)

Sparse weight by layer
Layer: conv1_1.weight, 0.47858796296296297 (827/1728)
Layer: conv2_1.weight, 0.5255940755208334 (38751/73728)
Layer: conv3_1.weight, 0.5858629014756944 (172778/294912)
Layer: fc1.weight, 0.7453654408454895 (12505157/16777216)
Layer: fc1.bias, 0.685302734375 (2807/4096)
Layer: fc2.weight, 0.486376953125 (19922/40960)
Layer: fc2.bias, 0.6 (6/10)


Sparse weight by filter
Layer: conv1_1.weight, [12, 13, 14, 7, 12, 12, 15, 10, 14, 11, 13, 14, 14, 12, 13, 13, 14, 15, 12, 12, 14, 13, 13, 13, 14, 14, 15, 13, 14, 11, 12, 15, 12, 5, 13, 16, 12, 13, 13, 14, 14, 12, 10, 11, 27, 14, 14, 11, 12, 15, 13, 14, 7, 13, 11, 15, 12, 13, 14, 12, 10, 16, 13, 13]
Layer: conv2_1.weight, [296, 576, 332, 379, 309, 319, 279, 214, 384, 345, 358, 309, 355, 277, 287, 274, 296, 282, 248, 316, 285, 297, 223, 297, 248, 326, 316, 377, 230, 264, 254, 290, 316, 309, 328, 273, 254, 243, 262, 255, 246, 347, 242, 272, 248, 326, 291, 325, 258, 281, 292, 313, 226, 229, 341, 309, 258, 380, 282, 237, 214, 275, 272, 328, 294, 576, 378, 316, 302, 354, 309, 258, 278, 249, 319, 349, 229, 308, 224, 347, 315, 282, 261, 261, 214, 271, 346, 576, 302, 576, 255, 298, 275, 242, 330, 330, 285, 272, 341, 369, 275, 359, 292, 250, 262, 351, 381, 255, 399, 302, 223, 245, 285, 306, 289, 255, 281, 242, 271, 305, 310, 320, 308, 358, 319, 337, 279, 327]
Layer: conv3_1.weight, [661, 572, 592, 606, 634, 1152, 659, 609, 560, 596, 627, 597, 627, 674, 550, 1152, 578, 612, 1152, 1152, 1139, 570, 589, 580, 601, 617, 642, 586, 600, 573, 578, 566, 606, 685, 610, 681, 588, 593, 639, 577, 623, 565, 672, 607, 518, 1152, 625, 582, 591, 558, 703, 594, 653, 546, 584, 651, 664, 588, 636, 649, 657, 648, 583, 639, 663, 605, 639, 657, 582, 570, 626, 580, 625, 588, 607, 629, 578, 595, 577, 592, 642, 623, 643, 633, 676, 586, 638, 605, 661, 591, 568, 559, 602, 613, 1152, 621, 648, 607, 636, 1152, 601, 556, 578, 1144, 655, 576, 622, 622, 615, 614, 629, 635, 602, 668, 615, 1152, 568, 588, 569, 634, 597, 659, 576, 628, 684, 557, 627, 661, 573, 597, 648, 593, 698, 605, 618, 627, 607, 599, 583, 602, 636, 637, 645, 566, 633, 628, 568, 597, 611, 621, 568, 564, 1152, 1145, 570, 595, 1152, 610, 650, 1152, 646, 612, 588, 553, 677, 641, 637, 566, 599, 572, 625, 1152, 1152, 697, 707, 607, 642, 578, 1152, 609, 602, 598, 626, 606, 603, 599, 597, 556, 1152, 602, 672, 604, 569, 642, 671, 1152, 591, 681, 654, 594, 599, 657, 635, 602, 663, 581, 1152, 633, 1152, 636, 1152, 769, 626, 1152, 1152, 633, 591, 597, 639, 593, 649, 647, 1152, 590, 614, 581, 607, 634, 1152, 669, 622, 585, 625, 1152, 603, 667, 674, 585, 591, 586, 1152, 632, 587, 614, 606, 600, 614, 1152, 594, 604, 609, 634, 690, 581, 625, 581]
Layer: fc1.weight, None
Layer: fc1.bias, None
Layer: fc2.weight, None
Layer: fc2.bias, None

Total sparse weights: 74.089 (12737855/17192650)

Sparse weight by layer
Layer: conv1_1.weight, 0.4791666666666667 (828/1728)
Layer: conv2_1.weight, 0.5214029947916666 (38442/73728)
Layer: conv3_1.weight, 0.5862291124131944 (172886/294912)
Layer: fc1.weight, 0.7452343702316284 (12502958/16777216)
Layer: fc1.bias, 0.68505859375 (2806/4096)
Layer: fc2.weight, 0.4865478515625 (19929/40960)
Layer: fc2.bias, 0.6 (6/10)



Sparse weight by filter
Layer: conv1_1.weight, [12, 13, 14, 7, 12, 12, 15, 10, 14, 11, 14, 14, 14, 12, 13, 13, 14, 15, 12, 12, 14, 13, 13, 13, 14, 14, 15, 13, 14, 11, 12, 15, 12, 5, 13, 16, 12, 13, 13, 14, 14, 12, 10, 11, 27, 14, 14, 11, 12, 15, 13, 14, 7, 13, 11, 15, 12, 13, 14, 12, 10, 16, 13, 13]
Layer: conv2_1.weight, [296, 576, 331, 379, 307, 319, 279, 214, 384, 345, 358, 309, 355, 279, 286, 274, 296, 282, 248, 316, 285, 298, 223, 297, 247, 326, 316, 377, 229, 264, 254, 290, 315, 309, 328, 273, 254, 243, 253, 254, 246, 347, 242, 272, 248, 326, 291, 326, 258, 281, 292, 313, 226, 229, 341, 309, 259, 379, 281, 235, 214, 275, 272, 328, 294, 352, 378, 316, 302, 354, 309, 258, 278, 249, 319, 349, 229, 308, 224, 347, 317, 282, 261, 260, 214, 271, 346, 513, 302, 576, 255, 298, 275, 242, 330, 329, 285, 272, 341, 369, 276, 359, 292, 250, 262, 351, 381, 248, 399, 298, 223, 245, 285, 306, 290, 255, 281, 245, 271, 305, 310, 320, 308, 358, 319, 336, 280, 327]
Layer: conv3_1.weight, [662, 572, 590, 608, 633, 1152, 662, 609, 577, 606, 625, 605, 627, 673, 561, 1152, 581, 608, 1127, 1152, 1084, 570, 588, 581, 606, 606, 638, 594, 603, 579, 594, 575, 606, 686, 609, 670, 591, 586, 647, 585, 622, 565, 673, 609, 526, 1152, 627, 592, 586, 566, 711, 595, 650, 552, 591, 645, 664, 594, 636, 648, 655, 648, 584, 638, 672, 596, 639, 656, 582, 587, 634, 582, 625, 596, 598, 643, 571, 586, 585, 586, 638, 621, 636, 643, 664, 579, 635, 617, 667, 595, 569, 560, 603, 613, 1152, 621, 648, 623, 625, 1152, 613, 555, 580, 1019, 644, 577, 611, 629, 613, 623, 630, 651, 608, 667, 615, 1152, 568, 591, 572, 634, 610, 659, 582, 620, 692, 562, 621, 658, 568, 591, 657, 590, 696, 606, 618, 626, 616, 597, 591, 595, 633, 633, 643, 559, 636, 632, 575, 592, 610, 613, 576, 562, 1152, 1104, 570, 603, 1152, 610, 661, 1152, 648, 622, 585, 559, 677, 642, 628, 566, 611, 580, 622, 1152, 1152, 704, 707, 598, 634, 584, 1152, 601, 614, 600, 634, 606, 608, 599, 601, 567, 1135, 600, 672, 608, 572, 641, 678, 1152, 599, 692, 659, 601, 608, 653, 620, 616, 671, 585, 1152, 645, 1152, 645, 1152, 770, 624, 1152, 1152, 639, 593, 612, 648, 595, 652, 662, 1152, 595, 619, 581, 608, 645, 1152, 687, 611, 587, 629, 1152, 603, 671, 683, 577, 609, 593, 1071, 626, 585, 621, 617, 600, 614, 1152, 596, 602, 618, 637, 669, 581, 618, 582]
Layer: fc1.weight, None
Layer: fc1.bias, None
Layer: fc2.weight, None
Layer: fc2.bias, None
Test: Loss: 1.857, Acc: 64.062 (82/128)
Final test accuracy: 64.0625
