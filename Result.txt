Result of weight pruning:
Size of the unpruned model before compression: 4.11 Mb
Size of the unpruned model after compression: 3.79 Mb
Size of the pruned model before compression: 16.28 Mb
Size of the pruned model after compression: 3.27 Mb

Unpruned Model:
Test loss: 0.9289465546607971
Test accuracy: 0.6873999834060669

Pruned Model:
Test loss: 0.9116255044937134
Test accuracy: 0.7014999985694885
313/313 [==============================] - 1s 2ms/step
313/313 [==============================] - 1s 2ms/step




Result of training model without applying Filter pruning:
Train: Loss: 1.843, ACC: 63.072 (31536/50000)
Train Time: 21.443
Test: Loss: 1.840, ACC: 63.374 (31687/50000)
Test Time: 16.215
Best Training Accuracy: 63.072%
Best Test Accuracy: 63.374%

Result of training model with Filter pruning and group lasso regularization:
Train: Loss: 0.035, ACC: 62.500 (50/80)
Train Time: 13.935
Test: Loss: 1.818, ACC: 66.406 (85/128)
Test Time: 0.041
Best Training Accuracy: 73.750%
Best Test Accuracy: 76.562%
Test: Loss: 1.857, ACC: 64.062 (82/128)
Final test accuracy: 64.0625

Total sparse weights: 74.089 (12737855/17192650)

Sparse weight by layer
Layer: conv1_1.weight, 0.4791666666666667 (828/1728)
Layer: conv2_1.weight, 0.5214029947916666 (38442/73728)
Layer: conv3_1.weight, 0.5862291124131944 (172886/294912)
Layer: fc1.weight, 0.7452343702316284 (12502958/16777216)
Layer: fc1.bias, 0.68505859375 (2806/4096)
Layer: fc2.weight, 0.4865478515625 (19929/40960)
Layer: fc2.bias, 0.6 (6/10)

Sparse weight by filter
Layer: conv1_1.weight, [12, 13, 14, 7, 12, 12, 15, 10, 14, 11, 14, 14, 14, 12, 13, 13, 14, 15, 12, 12, 14, 13, 13, 13, 14, 14, 15, 13, 14, 11, 12, 15, 12, 5, 13, 16, 12, 13, 13, 14, 14, 12, 10, 11, 27, 14, 14, 11, 12, 15, 13, 14, 7, 13, 11, 15, 12, 13, 14, 12, 10, 16, 13, 13]

Layer: conv2_1.weight, [296, 576, 331, 379, 307, 319, 279, 214, 384, 345, 358, 309, 355, 279, 286, 274, 296, 282, 248, 316, 285, 298, 223, 297, 247, 326, 316, 377, 229, 264, 254, 290, 315, 309, 328, 273, 254, 243, 253, 254, 246, 347, 242, 272, 248, 326, 291, 326, 258, 281, 292, 313, 226, 229, 341, 309, 259, 379, 281, 235, 214, 275, 272, 328, 294, 352, 378, 316, 302, 354, 309, 258, 278, 249, 319, 349, 229, 308, 224, 347, 317, 282, 261, 260, 214, 271, 346, 513, 302, 576, 255, 298, 275, 242, 330, 329, 285, 272, 341, 369, 276, 359, 292, 250, 262, 351, 381, 248, 399, 298, 223, 245, 285, 306, 290, 255, 281, 245, 271, 305, 310, 320, 308, 358, 319, 336, 280, 327]

Layer: conv3_1.weight, [662, 572, 590, 608, 633, 1152, 662, 609, 577, 606, 625, 605, 627, 673, 561, 1152, 581, 608, 1127, 1152, 1084, 570, 588, 581, 606, 606, 638, 594, 603, 579, 594, 575, 606, 686, 609, 670, 591, 586, 647, 585, 622, 565, 673, 609, 526, 1152, 627, 592, 586, 566, 711, 595, 650, 552, 591, 645, 664, 594, 636, 648, 655, 648, 584, 638, 672, 596, 639, 656, 582, 587, 634, 582, 625, 596, 598, 643, 571, 586, 585, 586, 638, 621, 636, 643, 664, 579, 635, 617, 667, 595, 569, 560, 603, 613, 1152, 621, 648, 623, 625, 1152, 613, 555, 580, 1019, 644, 577, 611, 629, 613, 623, 630, 651, 608, 667, 615, 1152, 568, 591, 572, 634, 610, 659, 582, 620, 692, 562, 621, 658, 568, 591, 657, 590, 696, 606, 618, 626, 616, 597, 591, 595, 633, 633, 643, 559, 636, 632, 575, 592, 610, 613, 576, 562, 1152, 1104, 570, 603, 1152, 610, 661, 1152, 648, 622, 585, 559, 677, 642, 628, 566, 611, 580, 622, 1152, 1152, 704, 707, 598, 634, 584, 1152, 601, 614, 600, 634, 606, 608, 599, 601, 567, 1135, 600, 672, 608, 572, 641, 678, 1152, 599, 692, 659, 601, 608, 653, 620, 616, 671, 585, 1152, 645, 1152, 645, 1152, 770, 624, 1152, 1152, 639, 593, 612, 648, 595, 652, 662, 1152, 595, 619, 581, 608, 645, 1152, 687, 611, 587, 629, 1152, 603, 671, 683, 577, 609, 593, 1071, 626, 585, 621, 617, 600, 614, 1152, 596, 602, 618, 637, 669, 581, 618, 582]



